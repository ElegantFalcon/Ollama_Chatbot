{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e1a805-8956-4ddf-94ff-a7ccd1391ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5022804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_bucket(chat_state, context_bucket):\n",
    "    if len(chat_state) < 2:\n",
    "        return context_bucket, \"‚ö†Ô∏è Not enough messages to save\"\n",
    "    last_user = chat_state[-2]\n",
    "    last_bot = chat_state[-1]\n",
    "    if last_user[\"role\"] != \"user\" or last_bot[\"role\"] != \"assistant\":\n",
    "        return context_bucket, \"‚ö†Ô∏è Improper roles\"\n",
    "    context_bucket.append(last_user)\n",
    "    context_bucket.append(last_bot)\n",
    "    return context_bucket, \"‚úÖ Saved to context bucket!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c25c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_context_display(context_bucket):\n",
    "    text = \"\"\n",
    "    for i in range(0, len(context_bucket), 2):\n",
    "        user = context_bucket[i][\"content\"]\n",
    "        bot = context_bucket[i+1][\"content\"] if i+1 < len(context_bucket) else \"\"\n",
    "        text += f\"üë§: {user}\\nü§ñ: {bot}\\n\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1ca3f1-7c0a-41c6-8436-57cda6d5e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLLAMA_API_BASE = \"http://localhost:11434\"\n",
    "\n",
    "def list_models():\n",
    "    \"\"\"Returns a list of installed models.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_API_BASE}/api/tags\")\n",
    "        response.raise_for_status()\n",
    "        models = response.json().get(\"models\", [])\n",
    "        return [model[\"name\"] for model in models]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"Static or dynamically fetched list of common/popular models to offer for download.\"\"\"\n",
    "    return [\n",
    "        \"llama2\", \"mistral\", \"gemma\", \"codellama\", \"orca-mini\", \n",
    "        \"phi\", \"dolphin-mixtral\", \"llava\", \"qwen\", \"tinyllama\"\n",
    "    ]\n",
    "\n",
    "\n",
    "def is_model_installed(model_name):\n",
    "    \"\"\"Check if a model is already pulled locally.\"\"\"\n",
    "    return model_name in list_models()\n",
    "\n",
    "def pull_model(model_name, progress_callback):\n",
    "    \"\"\"Pull a model using Ollama's API with progress updates.\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_API_BASE}/api/pull\",\n",
    "            json={\"name\": model_name},\n",
    "            stream=True\n",
    "        )\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                progress_callback(line.decode())\n",
    "    except Exception as e:\n",
    "        progress_callback(f\"Error pulling model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7063e91e-56d9-4471-8693-49998a6359cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_history = []\n",
    "def chat_with_model(message, model_name, chat_history_state, progress=gr.Progress(track_tqdm=True)):\n",
    "    if not is_model_installed(model_name):\n",
    "        permission = gr.update(visible=True)\n",
    "        return gr.update(value=\"Model not found locally. Please allow installation.\"), chat_history_state, permission\n",
    "\n",
    "    # Append user message\n",
    "    chat_history_state.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_API_BASE}/api/chat\",\n",
    "        json={\n",
    "            \"model\": model_name,\n",
    "            \"messages\": chat_history_state,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "    )\n",
    "    reply = response.json()[\"message\"][\"content\"]\n",
    "\n",
    "    # Append assistant reply\n",
    "    chat_history_state.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    formatted_history = []\n",
    "    for i in range(0, len(chat_history_state), 2):\n",
    "        user_msg = chat_history_state[i][\"content\"] if i < len(chat_history_state) else \"\"\n",
    "        assistant_msg = chat_history_state[i+1][\"content\"] if i+1 < len(chat_history_state) else \"\"\n",
    "        formatted_history.append([user_msg, assistant_msg])\n",
    "\n",
    "    return formatted_history, chat_history_state, gr.update(visible=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "271c5752-7af7-46cb-8ff1-5bd308b9a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def install_model_ui(model_name):\n",
    "    outputs = []\n",
    "    def pull_callback(update):\n",
    "        outputs.append(update)\n",
    "\n",
    "    thread = threading.Thread(target=pull_model, args=(model_name, pull_callback))\n",
    "    thread.start()\n",
    "    while thread.is_alive():\n",
    "        time.sleep(1)\n",
    "        yield \"\\n\".join(outputs)\n",
    "    yield \"\\n\".join(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aef4fd3-4c9d-43a8-a33b-17cafcf0fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üí¨ Chat with Ollama LLMs\")\n",
    "    \n",
    "\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            installed_models = list_models()\n",
    "            all_known_models = list_available_models()\n",
    "            models_to_download = sorted(list(set(all_known_models) - set(installed_models)))\n",
    "\n",
    "            with gr.Row():\n",
    "                installed_dropdown = gr.Dropdown(label=\"Installed Models\", choices=installed_models, interactive=True)\n",
    "                download_dropdown = gr.Dropdown(label=\"Available to Download\", choices=models_to_download, interactive=True)\n",
    "\n",
    "\n",
    "\n",
    "            chatbot = gr.Chatbot(label=\"Conversation\", type=\"messages\")\n",
    "            msg = gr.Textbox(label=\"Your Message\", placeholder=\"Type a message and press Enter\")\n",
    "            state = gr.State([])\n",
    "            with gr.Row():\n",
    "                save_button = gr.Button(\"üì• Save to Context Bucket\")\n",
    "            install_warning = gr.Textbox(value=\"Model not installed. Please install it first.\", visible=False, interactive=False)\n",
    "            install_button = gr.Button(\"Install Selected Model\", visible=False)\n",
    "            install_output = gr.Textbox(label=\"Installation Progress\", lines=6, visible=True)\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### üìú Chat History\")\n",
    "\n",
    "            chat_history_box = gr.Textbox(label=\"Raw History\", lines=20, interactive=False, show_copy_button=True)\n",
    "\n",
    "    def handle_user_input(user_message, selected_model, chat_state):\n",
    "        if not is_model_installed(selected_model):\n",
    "            return [], chat_state, gr.update(visible=True), \"\"\n",
    "\n",
    "        # Append user message to state\n",
    "        chat_state.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "        # Send request to Ollama\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{OLLAMA_API_BASE}/api/chat\",\n",
    "                json={\n",
    "                    \"model\": selected_model,\n",
    "                    \"messages\": chat_state,\n",
    "                    \"stream\": False\n",
    "                }\n",
    "            )\n",
    "            reply = response.json()[\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            reply = f\"‚ö†Ô∏è Error: {str(e)}\"\n",
    "    \n",
    "        # Append assistant's reply to state\n",
    "        chat_state.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    \n",
    "        # Format for gr.Chatbot (type=\"messages\")\n",
    "        chatbot_messages = [\n",
    "            {\"role\": entry[\"role\"], \"content\": entry[\"content\"]} for entry in chat_state\n",
    "        ]\n",
    "    \n",
    "        # Format for sidebar history\n",
    "        readable_text = \"\"\n",
    "        for i in range(0, len(chat_state), 2):\n",
    "            user = chat_state[i][\"content\"]\n",
    "            bot = chat_state[i + 1][\"content\"] if i + 1 < len(chat_state) else \"\"\n",
    "            readable_text += f\"üë§: {user}\\nü§ñ: {bot}\\n\\n\"\n",
    "    \n",
    "        return chatbot_messages, chat_state, gr.update(visible=False), readable_text\n",
    "\n",
    "\n",
    "    def on_model_select(selected_model):\n",
    "        if not is_model_installed(selected_model):\n",
    "            return gr.update(visible=True)\n",
    "        return gr.update(visible=False)\n",
    "\n",
    "    def refresh_model_lists():\n",
    "        installed = list_models()\n",
    "        all_known = list_available_models()\n",
    "        not_installed = sorted(list(set(all_known) - set(installed)))\n",
    "        return (\n",
    "            gr.update(choices=installed),\n",
    "            gr.update(choices=not_installed)\n",
    "        )\n",
    "\n",
    "    def install_model_ui(model_name):\n",
    "        outputs = []\n",
    "        def pull_callback(update):\n",
    "            outputs.append(update)\n",
    "\n",
    "        thread = threading.Thread(target=pull_model, args=(model_name, pull_callback))\n",
    "        thread.start()\n",
    "        while thread.is_alive():\n",
    "            time.sleep(1)\n",
    "            yield \"\\n\".join(outputs)\n",
    "        yield \"\\n\".join(outputs)\n",
    "\n",
    "    msg.submit(\n",
    "    handle_user_input,\n",
    "    inputs=[msg, installed_dropdown, state],\n",
    "    outputs=[chatbot, state, install_button, chat_history_box]\n",
    "    )\n",
    "\n",
    "\n",
    "    download_dropdown.change(on_model_select, inputs=download_dropdown, outputs=install_button)\n",
    "    install_button.click(install_model_ui, inputs=download_dropdown, outputs=install_output)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af450f5-619f-4375-8a0a-c3660291c13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95e645-dae6-4ad2-9b29-25ec61e0afb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

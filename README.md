ğŸ§  Local LLM Chatbot Interface with Gradio + Ollama

This project provides an interactive chatbot UI built with Gradio, using locally hosted LLMs via Ollama.
âœ¨ Features

    ğŸ”„ Dropdowns to switch models

        Installed models

        Downloadable models (with install prompt and progress)

    ğŸ’¬ Chatbot UI 
    ğŸ§¾ Sidebar with live chat history

    âœ… No cloud dependency â€” works fully locally

ğŸš€ Getting Started
1. Install Requirements

pip install gradio requests

Make sure Ollama is installed and running locally:

ollama serve

2. Run the Notebook

Launch the Jupyter notebook:

jupyter notebook

Open chatbot_with_sidebar.ipynb and run all cells.
ğŸ§ª Available Models

This UI automatically lists:

    âœ… Installed Models (via ollama list)

    â¬‡ï¸ Available Models to Download (from ollama.com/library)

When selecting a model not installed locally, you'll be prompted to install it with live progress.
ğŸ›  Tech Stack
Tool	Purpose
Gradio	Frontend chatbot UI
Ollama	Local model server
Python	Backend logic
requests	To interact with Ollama API
ğŸ“ File Structure

ğŸ“¦ ollama-chatbot
 â”£ ğŸ§  chatbot_with_sidebar.ipynb   # Main notebook
 â”£ ğŸ“„ README.md                    # This file
 â”— requirements.txt               # Optional, for pip installs

âœ… Example LLMs You Can Use

    llama3

    mistral

    gemma

    codellama

    neural-chat

Check Ollama's model library for full list.

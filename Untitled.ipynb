{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa705c9-0e22-4585-87b7-7766c508495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "import gradio as gr\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "OLLAMA_API_BASE = \"http://localhost:11434\"\n",
    "\n",
    "# Load summarization model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# In[2]:\n",
    "def list_models():\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_API_BASE}/api/tags\")\n",
    "        response.raise_for_status()\n",
    "        return [model[\"name\"] for model in response.json().get(\"models\", [])]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def list_available_models():\n",
    "    return [\n",
    "        \"llama2\", \"mistral\", \"gemma\", \"codellama\", \"orca-mini\", \n",
    "        \"phi\", \"dolphin-mixtral\", \"llava\", \"qwen\", \"tinyllama\"\n",
    "    ]\n",
    "\n",
    "def is_model_installed(model_name):\n",
    "    return model_name in list_models()\n",
    "\n",
    "def pull_model(model_name, progress_callback):\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_API_BASE}/api/pull\",\n",
    "            json={\"name\": model_name},\n",
    "            stream=True\n",
    "        )\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                progress_callback(line.decode())\n",
    "    except Exception as e:\n",
    "        progress_callback(f\"Error pulling model: {e}\")\n",
    "\n",
    "def install_model_ui(model_name):\n",
    "    outputs = []\n",
    "    def pull_callback(update):\n",
    "        outputs.append(update)\n",
    "    thread = threading.Thread(target=pull_model, args=(model_name, pull_callback))\n",
    "    thread.start()\n",
    "    while thread.is_alive():\n",
    "        time.sleep(1)\n",
    "        yield \"\\n\".join(outputs)\n",
    "    yield \"\\n\".join(outputs)\n",
    "\n",
    "def summarize_context(history):\n",
    "    full_text = \"\"\n",
    "    for msg in history:\n",
    "        role = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "        full_text += f\"{role}: {msg['content']}\\n\"\n",
    "\n",
    "    inputs = tokenizer(full_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=150,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# In[3]:\n",
    "def handle_user_input(user_message, selected_model, chat_state, context_summary):\n",
    "    if not is_model_installed(selected_model):\n",
    "        return [], chat_state, gr.update(visible=True), \"\", context_summary\n",
    "\n",
    "    # Append user input and get response\n",
    "    chat_state.append({\"role\": \"user\", \"content\": user_message})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_API_BASE}/api/chat\",\n",
    "            json={\n",
    "                \"model\": selected_model,\n",
    "                \"messages\": chat_state,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        reply = response.json()[\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        reply = f\"‚ö†Ô∏è Error: {str(e)}\"\n",
    "\n",
    "    chat_state.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    formatted_chat = [{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in chat_state]\n",
    "\n",
    "    # Update raw history\n",
    "    raw = \"\"\n",
    "    for i in range(0, len(chat_state), 2):\n",
    "        user = chat_state[i][\"content\"]\n",
    "        bot = chat_state[i + 1][\"content\"] if i + 1 < len(chat_state) else \"\"\n",
    "        raw += f\"üë§: {user}\\nü§ñ: {bot}\\n\\n\"\n",
    "\n",
    "    return formatted_chat, chat_state, gr.update(visible=False), raw, context_summary\n",
    "\n",
    "def save_to_context(chat_state):\n",
    "    try:\n",
    "        summary = summarize_context(chat_state)\n",
    "    except Exception as e:\n",
    "        summary = f\"‚ö†Ô∏è Error summarizing context: {e}\"\n",
    "    return summary\n",
    "\n",
    "def on_model_select(model_name):\n",
    "    return gr.update(visible=not is_model_installed(model_name))\n",
    "\n",
    "def refresh_model_lists():\n",
    "    installed = list_models()\n",
    "    all_known = list_available_models()\n",
    "    not_installed = sorted(set(all_known) - set(installed))\n",
    "    return (\n",
    "        gr.update(choices=installed),\n",
    "        gr.update(choices=not_installed)\n",
    "    )\n",
    "\n",
    "# In[4]:\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üí¨ Chat with Ollama LLMs + Context Bucket\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            installed_dropdown = gr.Dropdown(label=\"Installed Models\", choices=list_models(), interactive=True)\n",
    "            download_dropdown = gr.Dropdown(label=\"Available to Download\", choices=sorted(set(list_available_models()) - set(list_models())), interactive=True)\n",
    "\n",
    "            chatbot = gr.Chatbot(label=\"Conversation\", type=\"messages\")\n",
    "            msg = gr.Textbox(label=\"Your Message\", placeholder=\"Type a message and press Enter\")\n",
    "            state = gr.State([])              # Chat history\n",
    "            context_state = gr.State(\"\")      # Summarized context\n",
    "\n",
    "            save_button = gr.Button(\"üì• Save to Context Bucket\")\n",
    "            install_button = gr.Button(\"Install Selected Model\", visible=False)\n",
    "            install_output = gr.Textbox(label=\"Installation Progress\", lines=6, visible=True)\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### üìú Raw Chat History\")\n",
    "            chat_history_box = gr.Textbox(label=\"Raw History\", lines=15, interactive=False, show_copy_button=True)\n",
    "\n",
    "            gr.Markdown(\"### ü™£ Context Bucket (Summarized)\")\n",
    "            context_bucket_box = gr.Textbox(label=\"\", lines=10, interactive=False, show_copy_button=True)\n",
    "\n",
    "    # Event bindings\n",
    "    msg.submit(\n",
    "        handle_user_input,\n",
    "        inputs=[msg, installed_dropdown, state, context_state],\n",
    "        outputs=[chatbot, state, install_button, chat_history_box, context_bucket_box]\n",
    "    )\n",
    "\n",
    "    download_dropdown.change(on_model_select, inputs=download_dropdown, outputs=install_button)\n",
    "    install_button.click(install_model_ui, inputs=download_dropdown, outputs=install_output)\n",
    "    save_button.click(save_to_context, inputs=state, outputs=context_bucket_box)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f51c01-27b4-4dcf-a1d5-941dafdf24c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
